Algorithm Decision Tree
Input: maxDepth, numFeatures, data
Output: DecisionTree model

1. If all data points in the input data have the same class label, return a leaf node with that class label.
2. If the maximum depth has been reached or there are no more features to split on, return a leaf node with the majority class label.
3. Select a random subset of features to consider for splitting.
4. For each selected feature, find the best split threshold and corresponding information gain.
5. Select the feature with the highest information gain.
6. Split the input data into two subsets based on the selected feature and threshold.
7. Recursively build a decision tree for each subset of data, using the remaining features.
8. Return the decision tree model.

Function to find best split threshold:
Input: feature, data
Output: bestThreshold, bestInfoGain

1. Sort the data points by the selected feature.
2. Initialize the left and right class frequency counts to 0.
3. For each possible split threshold between adjacent data points:
    a. Move the corresponding data point from the right to the left subset.
    b. Update the class frequency counts for the left and right subsets.
    c. Calculate the information gain for the split based on the updated class frequency counts.
    d. If the information gain is higher than the current best, update the best information gain and threshold.
4. Return the best threshold and corresponding information gain.